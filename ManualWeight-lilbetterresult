# We'll assign weights and tweak the shape of the decision trees in the forest. 
#We’ll define weights manually, to be able to off-set that imbalance slightly.
#In our case we have 300 fraud to 7000 non-fraud cases, so by setting the weight ratio to 1:12, 
#we get to a 1/3 fraud to 2/3 non-fraud ratio, which is good enough for training the model on.
#Change the weight option to set the ratio to 1 to 12 for the non-fraud and fraud cases, and set the split criterion to 'entropy'.
#Set the maximum depth to 10.
# We will get Missed-> 14fraud, false+ --> 2 & correct fraud--> 77



model = RandomForestClassifier(bootstrap=True,
                               class_weight={0:1, 1:12},
                               criterion='entropy',
                               # Change depth of model
                               max_depth=10,
                               # Change the number of samples in leaf nodes
                               min_samples_leaf=10, 
                               # Change the number of trees to use
                               n_estimators=20,
                               n_jobs=-1,
                               random_state=5)

# Run the function get_model_results
get_model_results(X_train, y_train, X_test, y_test, model)
